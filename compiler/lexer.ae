use "lib/vector.ae"
use "compiler/tokens.ae"
use "compiler/utils.ae"

@compiler c_include "ctype.h"
def is_alpha(c: char): bool extern("isalpha")
def is_digit(c: char): bool extern("isdigit")
def is_alnum(c: char): bool extern("isalnum")

struct Lexer {
    source: string
    source_len: i32
    i: i32
    loc: Location
    seen_newline: bool
    tokens: &Vector
}

def Lexer::make(source: string, filename: string): Lexer {
    return Lexer(
        source,
        source_len: source.len(),
        i: 0,
        loc: Location(filename, 1, 1),
        seen_newline: false,
        tokens: Vector::new()
    )
}

def Lexer::push(&this, token: &Token) {
    token.seen_newline = .seen_newline
    .tokens.push(token)
    .seen_newline = false
}

def Lexer::push_type(&this, type: TokenType, len: i32) {
    let start_loc = .loc
    .loc.col += len
    .i += len
    .push(Token::from_type(type, Span(start_loc, .loc)))
}

def Lexer::peek(&this, offset: i32): char {
    if .source[.i] == '\0' {
        return .source[.i]
    }
    return .source[.i + 1]
}

def Lexer::lex_char_literal(&this) {
    let start_loc = .loc
    let start = .i + 1
    .i += 1

    if .source[.i] == '\\' {
        .i += 2
    } else {
        .i += 1
    }
    if .source[.i] != '\'' {
        .loc.col += .i - start + 1
        error_loc(.loc, "Expected ' after character literal")
    }

    let len = .i - start
    let text = .source.substring(start, len)

    .loc.col += len + 2
    .i += 1
    .push(Token::new(TokenType::CharLiteral, Span(start_loc, .loc), text))
}

def Lexer::lex_string_literal(&this) {
    let start_loc = .loc
    let end_char = .source[.i]
    let start = .i + 1
    .i += 1
    while .source[.i] != end_char {
        if .source[.i] == '\\' {
            .i += 1
        }
        .i += 1
    }

    let len = .i - start
    let text = .source.substring(start, len)
    .loc.col += len + 2
    .i += 1
    if end_char == '`' {
        .push(Token::new(TokenType::FormatStringLiteral, Span(start_loc, .loc), text))
    } else {
        .push(Token::new(TokenType::StringLiteral, Span(start_loc, .loc), text))
    }
}

def Lexer::lex_int_literal_different_base(&this) {
    let start_loc = .loc
    let start = .i
    .i += 1
    match .source[.i] {
        'x' => {
            .i += 1
            // FIXME: check for invalid hex digits
            while .i < .source_len and is_alnum(.source[.i]) {
                .i += 1
            }
        }
        'b' => {
            .i += 1
            while .i < .source_len and .source[.i] == '0' or .source[.i] == '1' {
                .i += 1
            }
        }

        // Should be unreachable
        else => {}
    }
    let len = .i - start
    let text = .source.substring(start, len)
    .loc.col += len
    .push(Token::new(TokenType::IntLiteral, Span(start_loc, .loc), text))
}

def Lexer::lex_numeric_literal(&this) {
    let start_loc = .loc
    if .source[.i] == '0' {
        match .peek(1) {
            'x' | 'b' => {
                .lex_int_literal_different_base()
                return
            }
            // Do nothing, fall through
            else => {}
        }
    }

    let start = .i
    let token_type: TokenType
    while is_digit(.source[.i]) {
        .i += 1
    }
    if .source[.i] == '.' {
        .i += 1
        while is_digit(.source[.i]) {
            .i += 1
        }
        token_type = TokenType::FloatLiteral
    } else {
        token_type = TokenType::IntLiteral
    }
    let len = .i - start
    let text = .source.substring(start, len)

    .loc.col += len
    .push(Token::new(token_type, Span(start_loc, .loc), text))
}

def Lexer::lex(&this): &Vector {
    while .i < .source_len {
        let c = .source[.i]
        match c {
            ' ' | '\t' | '\v' | '\r' | '\b' => {
                .loc.col += 1
                .i += 1
            }
            '\n' => {
                .loc.line += 1
                .loc.col = 1
                .i += 1
                .seen_newline = true
            }
            ';' => .push_type(TokenType::Semicolon, len: 1)
            ',' => .push_type(TokenType::Comma, len: 1)
            '.' => .push_type(TokenType::Dot, len: 1)
            '(' => .push_type(TokenType::OpenParen, len: 1)
            ')' => .push_type(TokenType::CloseParen, len: 1)
            '[' => .push_type(TokenType::OpenSquare, len: 1)
            ']' => .push_type(TokenType::CloseSquare, len: 1)
            '{' => .push_type(TokenType::OpenCurly, len: 1)
            '}' => .push_type(TokenType::CloseCurly, len: 1)
            '@' => .push_type(TokenType::AtSign, len: 1)
            '%' => .push_type(TokenType::Percent, len: 1)
            '^' => .push_type(TokenType::Caret, len: 1)
            '&' => .push_type(TokenType::Ampersand, len: 1)
            '|' => .push_type(TokenType::Line, len: 1)
            '?' => .push_type(TokenType::Question, len: 1)
            '!' => match .peek(1) {
                '='  => .push_type(TokenType::NotEquals, len: 2)
                else => .push_type(TokenType::Exclamation, len: 1)
            }
            ':' => match .peek(1) {
                ':'  => .push_type(TokenType::ColonColon, len: 2)
                else => .push_type(TokenType::Colon, len: 1)
            }
            '=' => match .peek(1) {
                '='  => .push_type(TokenType::EqualEquals, len: 2)
                '>'  => .push_type(TokenType::FatArrow, len: 2)
                else => .push_type(TokenType::Equals, len: 1)
            }
            '*' => match .peek(1) {
                '='  => .push_type(TokenType::StarEquals, len: 2)
                else => .push_type(TokenType::Star, len: 1)
            }
            '+' => match .peek(1) {
                '='  => .push_type(TokenType::PlusEquals, len: 2)
                else => .push_type(TokenType::Plus, len: 1)
            }
            '-' => match .peek(1) {
                '='  => .push_type(TokenType::MinusEquals, len: 2)
                else => .push_type(TokenType::Minus, len: 1)
            }
            '<' => match .peek(1) {
                '='  => .push_type(TokenType::LessThanEquals, len: 2)
                '<'  => .push_type(TokenType::LessThanLessThan, len: 2)
                else => .push_type(TokenType::LessThan, len: 1)
            }
            '>' => match .peek(1) {
                '='  => .push_type(TokenType::GreaterThanEquals, len: 2)
                '>'  => .push_type(TokenType::GreaterThanGreaterThan, len: 2)
                else => .push_type(TokenType::GreaterThan, len: 1)
            }
            '/' => match .peek(1) {
                '/' => {
                    .i += 1
                    while .i < .source_len and .source[.i] != '\n' {
                        .i += 1
                    }
                }
                '='  => .push_type(TokenType::SlashEquals, len: 2)
                else => .push_type(TokenType::Slash, len: 1)
            }
            '\'' => .lex_char_literal()
            '"' | '`' => .lex_string_literal()
            else => {
                let start_loc = .loc

                if is_digit(c) {
                    .lex_numeric_literal()

                } else if is_alpha(c) or c == '_' {
                    let start = .i
                    while is_alnum(.source[.i]) or .source[.i] == '_' {
                        .i += 1
                    }
                    let len = .i - start
                    let text = .source.substring(start, len)

                    .loc.col += len
                    .push(Token::from_ident(text, Span(start_loc, .loc)))

                } else {
                    panic(`{.loc.str()}: Unrecognized char in lexer: '{c}'`)
                }
            }
        }
    }

    // We can assume EOF acts like a newline
    .seen_newline = true
    .push_type(TokenType::EOF, len: 0)
    return .tokens
}
